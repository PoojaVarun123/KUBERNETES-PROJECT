================================================================================================================================================
1) Set up EBS CSI Driver on Kubernetes using Helm
================================================================================================================================================
The Amazon EBS CSI driver allows your Kubernetes workloads to use EBS volumes as persistent storage.
üõ†Ô∏è Steps:
Step 1: Add the EBS CSI driver Helm repo
    helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    helm repo update
----------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Install the driver using Helm (with IRSA if on EKS)
  helm upgrade --install aws-ebs-csi-driver \
  aws-ebs-csi-driver/aws-ebs-csi-driver \
  --namespace kube-system \
  --set controller.serviceAccount.create=false \
  --set controller.serviceAccount.name=ebs-csi-controller-sa
If using IRSA, make sure the IAM Role has the following policy attached:
    AWS EBS CSI IAM policy
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Create a StorageClass
# ebs-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer

kubectl apply -f ebs-sc.yaml
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 4: Use the StorageClass in a Deployment or StatefulSet
# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ebs-sc
  resources:
    requests:
      storage: 5Gi
________________________________________
‚úÖ 2) Set up EFS CSI Driver on Kubernetes using Helm
EFS provides scalable file storage that can be shared across pods.
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Steps:
Step 1: Add the EFS Helm repo
    helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Install the EFS CSI Driver
    helm upgrade --install aws-efs-csi-driver \
  aws-efs-csi-driver/aws-efs-csi-driver \
  --namespace kube-system
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Create an EFS File System on AWS
‚Ä¢	Create via AWS Console or CLI.
‚Ä¢	Note the FileSystemId.
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 4: Create EFS Access Point (optional but recommended)
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 5: Create a StorageClass and PersistentVolume
# efs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: <file-system-id>

# efs-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
yaml
CopyEdit
# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi
________________________________________
‚úÖ 3) Set up AWS CNI Plugin (VPC CNI) using Helm
The AWS CNI plugin is the default networking plugin for EKS.
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Steps:
Step 1: Add the Amazon VPC CNI Helm repo
    helm repo add eks https://aws.github.io/eks-charts
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Install the CNI plugin
    helm upgrade --install aws-vpc-cni \
  eks/aws-vpc-cni \
  --namespace kube-system \
  --set init.enabled=true
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: IAM Permissions (if using IRSA)
    Attach the AmazonEKS_CNI_Policy managed policy to the node IAM role.
________________________________________
‚úÖ 4) Set up Calico Network Plugin on Kubernetes using Helm
Calico provides advanced network policies and optionally replaces the default CNI plugin.
üõ†Ô∏è Steps:
Calico is often used with self-managed Kubernetes, not EKS unless you disable AWS CNI.
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 1: Add the Project Calico Helm repo
    helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Install Calico using Helm
helm install calico projectcalico/tigera-operator \
  --version v3.27.0 \
  --namespace tigera-operator \
  --create-namespace
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Wait for Calico to be ready
    kubectl get pods -n calico-system
You can also define custom network policies using Calico after it's deployed.
________________________________________
‚úÖ Summary Table
| Component      | Helm Chart Repo                                        | Helm Install Command Example          |
| -------------- | ------------------------------------------------------ | ------------------------------------- |
| EBS CSI Driver | `https://kubernetes-sigs.github.io/aws-ebs-csi-driver` | `helm install aws-ebs-csi-driver ...` |
| EFS CSI Driver | `https://kubernetes-sigs.github.io/aws-efs-csi-driver` | `helm install aws-efs-csi-driver ...` |
| AWS CNI Plugin | `https://aws.github.io/eks-charts`                     | `helm install aws-vpc-cni ...`        |
| Calico Network | `https://projectcalico.docs.tigera.io/charts`          | `helm install calico ...`             |
________________________________________
5) How to Set Up Istio Service Mesh using Helm
üõ†Ô∏è Steps:
Step 1: Add Istio Helm repo
    helm repo add istio https://istio-release.storage.googleapis.com/charts
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Create the namespace
    kubectl create namespace istio-system
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Install Istio base components
    helm install istio-base istio/base -n istio-system
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 4: Install Istio control plane (istiod)
    helm install istiod istio/istiod -n istio-system --wait
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 5: (Optional) Install Istio Ingress Gateway
    helm install istio-ingress istio/gateway -n istio-system
________________________________________
‚úÖ 6) How to Set Up Horizontal Pod Autoscaler (HPA)
üõ†Ô∏è Prerequisites:
‚Ä¢	Metrics Server must be installed:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Example HPA YAML:
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
________________________________________
‚úÖ 7) How to Set Up Cluster Autoscaler on AWS EKS
üõ†Ô∏è Prerequisites:
‚Ä¢	Enable IAM Role with Cluster Autoscaler permissions using IRSA.
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Step-by-step (with Helm):
Step 1: Add Helm repo
    helm repo add autoscaler https://kubernetes.github.io/autoscaler
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Create IAM role with Cluster Autoscaler policy (via IRSA).
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Install Cluster Autoscaler via Helm
  helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set autoDiscovery.clusterName=<your-cluster-name> \
  --set awsRegion=<region> \
  --set rbac.serviceAccount.name=cluster-autoscaler \
  --set rbac.serviceAccount.create=false \
  --set cloudProvider=aws \
  --set extraArgs.balance-similar-node-groups=true
You can also pass --set extraArgs.expander=least-waste for better node group decisions.
________________________________________
‚úÖ 8) How to Set Up Vertical Pod Autoscaler (VPA)
üõ†Ô∏è Install VPA using YAML:
    kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Example VPA YAML:
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       myapp
  updatePolicy:
    updateMode: "Auto"
________________________________________
‚úÖ 9) How to Integrate AWS Secrets Manager into Kubernetes
üõ†Ô∏è Use Secrets Store CSI Driver + AWS Provider
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 1: Install Secrets Store CSI Driver (Helm)
      helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts
      helm repo update
    helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver \
  --namespace kube-system
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Install AWS provider
    kubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Create SecretProviderClass
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: aws-secrets
spec:
  provider: aws
  parameters:
    objects: |
      - objectName: "my-secret-name"
        objectType: "secretsmanager"
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 4: Use in Pod
volumeMounts:
- name: secrets-store-inline
  mountPath: "/mnt/secrets-store"

volumes:
- name: secrets-store-inline
  csi:
    driver: secrets-store.csi.k8s.io
    readOnly: true
    volumeAttributes:
      secretProviderClass: "aws-secrets"
________________________________________
‚úÖ 10) How to Set Up AWS ALB Ingress Controller using Helm
üõ†Ô∏è Steps:
Step 1: Add Helm repo
    helm repo add eks https://aws.github.io/eks-charts
    helm repo update
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 2: Create IAM role for ALB controller (IRSA)
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 3: Install ALB Controller
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=<your-cluster-name> \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=<aws-region> \
  --set vpcId=<vpc-id> \
  --set image.tag="v2.7.1"
---------------------------------------------------------------------------------------------------------------------------------------------------------
Step 4: Annotate Ingress Resource
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 80
________________________________________
‚úÖ 11) How to Upgrade Kubernetes Worker Nodes on EKS
üõ†Ô∏è Steps (Managed Node Group):
1.	Check current version:
    aws eks describe-nodegroup --cluster-name <cluster> --nodegroup-name <ng-name>
---------------------------------------------------------------------------------------------------------------------------------------------------------
2.	Upgrade Node Group:
aws eks update-nodegroup-version \
  --cluster-name <cluster-name> \
  --nodegroup-name <nodegroup-name> \
  --kubernetes-version <new-version>
---------------------------------------------------------------------------------------------------------------------------------------------------------
3.	Monitor upgrade status:
    aws eks describe-nodegroup ...
---------------------------------------------------------------------------------------------------------------------------------------------------------
üõ†Ô∏è Steps (Self-managed nodes with ASG):
1.	Launch new node group with updated AMI.
2.	Drain old nodes:
    kubectl drain <node> --ignore-daemonsets --delete-local-data
3.	Terminate old ASG instances.
4.	Confirm pods are running on new nodes.
________________________________________
________________________________________
üîß List of Components We‚Äôll Cover:
1.	EBS CSI Driver
2.	EFS CSI Driver
3.	AWS CNI (VPC CNI)
4.	Calico
5.	Istio
6.	HPA (Horizontal Pod Autoscaler)
7.	Cluster Autoscaler
8.	Vertical Pod Autoscaler
9.	AWS Secrets Manager CSI Integration
10.	ALB Ingress Controller
11.	Node Upgrade (EKS managed & self-managed)
________________________________________
________________________________________
‚úÖ 1. Amazon EBS CSI Driver Setup
________________________________________
üß© A. YAML Manifests
1. StorageClass YAML (ebs-sc.yaml)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
parameters:
  type: gp3
---------------------------------------------------------------------------------------------------------------------------------------------------------
2. PersistentVolumeClaim YAML (ebs-pvc.yaml)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ebs-sc
  resources:
    requests:
      storage: 5Gi
---------------------------------------------------------------------------------------------------------------------------------------------------------
3. Sample Pod using EBS PVC (ebs-pod.yaml)
apiVersion: v1
kind: Pod
metadata:
  name: ebs-demo-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: ebs-volume
  volumes:
  - name: ebs-volume
    persistentVolumeClaim:
      claimName: ebs-pvc
________________________________________
üì¶ B. Helm values.yaml (For EBS CSI driver)
ebs-csi-values.yaml
controller:
  serviceAccount:
    create: false
    name: ebs-csi-controller-sa
node:
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
storageClasses:
  enabled: false
You need to create the IAM Role + ServiceAccount separately if you're using IRSA.
________________________________________
‚öôÔ∏è C. Terraform Code
1. IAM Policy for EBS CSI (required for IRSA)
resource "aws_iam_policy" "ebs_csi_policy" {
  name = "AmazonEBSCSIDriverPolicy"
  policy = file("ebs-csi-iam-policy.json")
}
---------------------------------------------------------------------------------------------------------------------------------------------------------
2. IAM Role for ServiceAccount (IRSA)
resource "aws_iam_role" "ebs_csi_role" {
  name = "eks-ebs-csi-role"
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data "aws_iam_policy_document" "assume_role" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.oidc.arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.oidc.url, "https://", "")}:sub"
      values   = ["system:serviceaccount:kube-system:ebs-csi-controller-sa"]
    }
  }
}
---------------------------------------------------------------------------------------------------------------------------------------------------------
3. Associate IAM Policy
resource "aws_iam_role_policy_attachment" "ebs_policy_attach" {
  policy_arn = aws_iam_policy.ebs_csi_policy.arn
  role       = aws_iam_role.ebs_csi_role.name
}
---------------------------------------------------------------------------------------------------------------------------------------------------------
4. Create the Kubernetes Service Account
resource "kubernetes_service_account" "ebs_sa" {
  metadata {
    name      = "ebs-csi-controller-sa"
    namespace = "kube-system"
    annotations = {
      "eks.amazonaws.com/role-arn" = aws_iam_role.ebs_csi_role.arn
    }
  }
}
________________________________________
üß™ Deployment Steps:
1. Create IRSA role and Kubernetes SA via Terraform
    terraform init && terraform apply
2. Install EBS CSI driver using Helm
    helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    helm repo update
    helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
  -n kube-system \
  -f ebs-csi-values.yaml
3. Apply YAML resources
kubectl apply -f ebs-sc.yaml
kubectl apply -f ebs-pvc.yaml
kubectl apply -f ebs-pod.yaml
________________________________________
‚úÖ 2. Amazon EFS CSI Driver Setup
________________________________________
üß© A. YAML Manifests
1. StorageClass YAML (efs-sc.yaml)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer

2. PersistentVolume YAML (efs-pv.yaml)
Replace <file-system-id> with your actual EFS File System ID.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-<file-system-id>

3. PersistentVolumeClaim YAML (efs-pvc.yaml)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi

4. Sample Pod using EFS PVC (efs-pod.yaml)
apiVersion: v1
kind: Pod
metadata:
  name: efs-demo-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/data"
      name: efs-volume
  volumes:
  - name: efs-volume
    persistentVolumeClaim:
      claimName: efs-pvc
________________________________________
üì¶ B. Helm values.yaml (for EFS CSI driver)
efs-csi-values.yaml
controller:
  region: <your-region>
  serviceAccount:
    create: true
    name: efs-csi-controller-sa
node:
  tolerations: []
If using IRSA, create your own ServiceAccount and attach an IAM role with EFS permissions.
________________________________________
‚öôÔ∏è C. Terraform Code
1. Create EFS File System
resource "aws_efs_file_system" "efs" {
  creation_token = "my-efs"
  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"
  tags = {
    Name = "my-eks-efs"
  }
}

2. Create Mount Targets
resource "aws_efs_mount_target" "efs_mount" {
  for_each = toset(var.private_subnet_ids)

  file_system_id  = aws_efs_file_system.efs.id
  subnet_id       = each.value
  security_groups = [aws_security_group.efs_sg.id]
}

3. IAM Policy for EFS CSI Driver
resource "aws_iam_policy" "efs_csi_policy" {
  name   = "AmazonEFSCSIDriverPolicy"
  policy = file("efs-csi-iam-policy.json") # Use official policy
}

4. IAM Role + IRSA setup (same as EBS setup, adjust service account name to efs-csi-controller-sa)
________________________________________
üß™ Deployment Steps:
Step 1: Deploy Terraform
    terraform init && terraform apply

Step 2: Add Helm repo and install EFS CSI
    helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/
    helm repo update
    helm install aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \
  -n kube-system \
  -f efs-csi-values.yaml

Step 3: Apply all YAMLs
    kubectl apply -f efs-sc.yaml
    kubectl apply -f efs-pv.yaml
    kubectl apply -f efs-pvc.yaml
    kubectl apply -f efs-pod.yaml
________________________________________
‚úÖ 3. AWS CNI Plugin (Amazon VPC CNI) Setup
The Amazon VPC CNI plugin is the default networking plugin used in EKS. It assigns VPC IP addresses to Kubernetes pods using ENIs 
(Elastic Network Interfaces).
________________________________________
üß© A. YAML Manifests
No direct YAMLs are typically needed, as this is a daemonset managed by Helm or EKS itself. However, custom configurations can be
made via ConfigMap:

Example: Customize the VPC CNI config
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-node
  namespace: kube-system
data:
  ENABLE_PREFIX_DELEGATION: "true"
  WARM_IP_TARGET: "5"
Apply using:
kubectl apply -f custom-cni-config.yaml
________________________________________
üì¶ B. Helm values.yaml for VPC CNI
Customize these settings as per your cluster requirements.
aws-vpc-cni-values.yaml
env:
  AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG: "false"
  ENABLE_PREFIX_DELEGATION: "true"
  WARM_IP_TARGET: "5"
  LOG_LEVEL: DEBUG

init:
  enabled: true

serviceAccount:
  create: false
  name: aws-node-sa
________________________________________
‚öôÔ∏è C. Terraform Code
1. IAM Policy for VPC CNI Plugin
Use the AWS managed policy: AmazonEKS_CNI_Policy
You can attach it like this:
resource "aws_iam_role_policy_attachment" "cni_attach" {
  role       = aws_iam_role.cni_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

2. IAM Role + IRSA for aws-node
hcl
CopyEdit
resource "aws_iam_role" "cni_role" {
  name = "eks-cni-irsa-role"

  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data "aws_iam_policy_document" "assume_role" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.oidc.arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.oidc.url, "https://", "")}:sub"
      values   = ["system:serviceaccount:kube-system:aws-node-sa"]
    }
  }
}

3. Kubernetes Service Account with IRSA annotation
resource "kubernetes_service_account" "aws_node_sa" {
  metadata {
    name      = "aws-node-sa"
    namespace = "kube-system"
    annotations = {
      "eks.amazonaws.com/role-arn" = aws_iam_role.cni_role.arn
    }
  }
}
________________________________________
üß™ Deployment Steps:
Step 1: Deploy Terraform to create IRSA role and SA
    terraform init && terraform apply

Step 2: Add Helm repo and install plugin
    helm repo add eks https://aws.github.io/eks-charts
    helm repo update

Step 3: Install or upgrade AWS VPC CNI plugin using Helm
helm upgrade --install aws-vpc-cni \
  eks/aws-vpc-cni \
  -n kube-system \
  -f aws-vpc-cni-values.yaml

Step 4: (Optional) Verify pods are using VPC IPst
    kubectl get pods -o wide
You should see pods assigned VPC subnet IPs directly.
________________________________________
‚úÖ 4. Calico Network Plugin Setup (Self-managed Kubernetes)
Calico is a powerful CNI plugin that provides networking and network policy enforcement in Kubernetes.
________________________________________
üß© A. YAML Manifests
1. Calico Default Installation (YAML method)
For quick installation:
    kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml
However, since you requested Helm + Terraform + YAML, we‚Äôll go with Helm.
________________________________________
üì¶ B. Helm-Based Setup
Step 1: Add Calico Helm repo
    helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
    helm repo update

Step 2: Create the required namespace
    kubectl create namespace tigera-operator

Step 3: Helm install command
helm install calico projectcalico/tigera-operator \
  --version v3.27.0 \
  --namespace tigera-operator
________________________________________
calico-values.yaml (optional configuration)
installation:
  kubernetesProvider: "k8s"
  cni:
    type: Calico
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 192.168.0.0/16
      encapsulation: VXLAN
      natOutgoing: Enabled
      nodeSelector: all()
Use via:
helm upgrade --install calico projectcalico/tigera-operator \
  --namespace tigera-operator \
  -f calico-values.yaml
________________________________________
‚öôÔ∏è C. Terraform Integration
You can provision Helm + Calico using Terraform‚Äôs Helm provider.
1. Helm Provider Block
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

2. Install Calico via Helm Chart
resource "helm_release" "calico" {
  name       = "calico"
  repository = "https://projectcalico.docs.tigera.io/charts"
  chart      = "tigera-operator"
  version    = "v3.27.0"
  namespace  = "tigera-operator"
  create_namespace = true

  values = [
    file("calico-values.yaml")
  ]
}
________________________________________
üîí Optional Network Policy YAML
Example: Deny all traffic by default
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: default
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
________________________________________
‚úÖ 5. Istio Service Mesh Setup (with YAML, Helm, and Terraform)
Istio is a powerful open-source service mesh that enables traffic management, observability, security, and policy enforcement between 
microservices in Kubernetes.
________________________________________
üß© A. YAML-Based Installation (Istioctl method)
Istio can be installed using the istioctl binary. However, Helm is the preferred approach for better customizability.
Step 1: Download Istio
  curl -L https://istio.io/downloadIstio | sh -
  cd istio-1.22.0
  export PATH=$PWD/bin:$PATH

Step 2: Install Istio base + istiod + ingress gateway
    istioctl install --set profile=demo -y
________________________________________
üì¶ B. Helm-Based Istio Setup
Step 1: Add Helm repo
    helm repo add istio https://istio-release.storage.googleapis.com/charts
    helm repo update

Step 2: Create istio-system namespace
    kubectl create namespace istio-system
Step 3: Install Istio components using Helm
________________________________________
1. Install istio-base
helm install istio-base istio/base \
  -n istio-system
________________________________________
2. Install istiod (control plane)
helm install istiod istio/istiod \
  -n istio-system \
  --wait \
  --set global.istioNamespace=istio-system
________________________________________
3. Install Istio ingress gateway
helm install istio-ingress istio/gateway \
  -n istio-system \
  --wait
________________________________________
Optional: istio-values.yaml Example
global:
  istioNamespace: istio-system

meshConfig:
  enablePrometheusMerge: true

pilot:
  autoscaleEnabled: true

values:
  telemetry:
    enabled: true

Use it via:
helm upgrade --install istiod istio/istiod \
  -n istio-system \
  -f istio-values.yaml
________________________________________
‚öôÔ∏è C. Terraform-Based Setup
1. Provider Setup
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}
________________________________________
2. Helm Releases via Terraform
resource "helm_release" "istio_base" {
  name       = "istio-base"
  repository = "https://istio-release.storage.googleapis.com/charts"
  chart      = "base"
  namespace  = "istio-system"
  create_namespace = true
}

resource "helm_release" "istiod" {
  name       = "istiod"
  repository = "https://istio-release.storage.googleapis.com/charts"
  chart      = "istiod"
  namespace  = "istio-system"
  depends_on = [helm_release.istio_base]

  values = [file("istio-values.yaml")]
}

resource "helm_release" "istio_ingress" {
  name       = "istio-ingress"
  repository = "https://istio-release.storage.googleapis.com/charts"
  chart      = "gateway"
  namespace  = "istio-system"
  depends_on = [helm_release.istiod]
}
________________________________________
üåê Istio Sidecar Injection
Enable automatic sidecar injection for a namespace:
    kubectl label namespace default istio-injection=enabled
________________________________________
‚úÖ Sample Service Deployment YAML with Istio Proxy
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
    - port: 80
      targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: nginx
          ports:
            - containerPort: 8080
If namespace has istio-injection=enabled, Istio will automatically inject the Envoy proxy.
________________________________________
‚úÖ 4. Calico Network Plugin Setup in Kubernetes (Helm + YAML + Terraform)
________________________________________
üîπ A. Definition
Calico is a CNI plugin that provides networking and network security (via NetworkPolicies) for Kubernetes clusters. It can operate in BGP 
mode or overlay mode and supports IP-in-IP, VXLAN, and wireguard encryption.
________________________________________
üîπ B. Installation via Helm
Step 1: Add Calico Helm repo
helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
helm repo update

Step 2: Install Calico with Helm
helm install calico projectcalico/tigera-operator \
  --namespace tigera-operator \
  --create-namespace
________________________________________
üîπ C. values.yaml for Calico (Customizable)
Here‚Äôs a sample calico-values.yaml (optional, can also use defaults):
installation:
  calicoNetwork:
    bgp: Disabled
    ipPools:
    - blockSize: 26
      cidr: 192.168.0.0/16
      encapsulation: VXLAN
      natOutgoing: Enabled
      nodeSelector: all()
To apply:
helm upgrade --install calico projectcalico/tigera-operator \
  --namespace tigera-operator \
  --values calico-values.yaml
________________________________________
üîπ D. Terraform Setup (Helm Provider)
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

resource "helm_release" "calico" {
  name       = "calico"
  namespace  = "tigera-operator"
  repository = "https://projectcalico.docs.tigera.io/charts"
  chart      = "tigera-operator"
  create_namespace = true

  values = [file("${path.module}/calico-values.yaml")]
}
________________________________________
üîπ E. Custom Calico Network Policy YAML (Example)
This allows only traffic within same namespace:
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: default
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
________________________________________
üîπ F. Importance
‚Ä¢	Enables fine-grained network security via Kubernetes NetworkPolicies.
‚Ä¢	Helps in multi-tenant clusters.
‚Ä¢	Works with or without overlay networking.
‚Ä¢	Integrates with Prometheus, WireGuard, and other advanced options.
________________________________________
üîπ G. Interviewer Summary
Calico is a powerful CNI plugin that offers both networking and security using standard Kubernetes resources. It‚Äôs deployed via Helm 
(tigera-operator) and is customizable via values.yaml or Terraform Helm provider. Calico is crucial in production-grade clusters for
enforcing pod-level isolation and compliance policies.
________________________________________
‚úÖ 5. Istio Service Mesh Setup in Kubernetes (Helm + YAML + Terraform)
________________________________________
üîπ A. Definition
Istio is a powerful service mesh platform that provides traffic management, observability, security, and policy enforcement for 
microservices running in Kubernetes. It uses Envoy proxies as sidecars to intercept and manage traffic between services.
________________________________________
üîπ B. Installation via Helm
Step 1: Download Istio CLI
curl -L https://istio.io/downloadIstio | sh -
cd istio-*/bin
export PATH=$PWD:$PATH

Step 2: Install Istio Base & Control Plane via Helm
helm repo add istio https://istio-release.storage.googleapis.com/charts
helm repo update

Install Istio base:
helm install istio-base istio/base -n istio-system --create-namespace

Install Istiod (control plane):
helm install istiod istio/istiod -n istio-system
________________________________________
üîπ C. Sample values.yaml for Istiod (Customizable)
global:
  proxy:
    logLevel: warning
  meshID: mesh1
  multiCluster:
    enabled: false
  network: network1
pilot:
  autoscaleEnabled: true

Apply with:
helm upgrade --install istiod istio/istiod \
  --namespace istio-system \
  --values istio-values.yaml
________________________________________
üîπ D. Terraform Setup (Helm Provider)
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

resource "helm_release" "istio_base" {
  name       = "istio-base"
  namespace  = "istio-system"
  repository = "https://istio-release.storage.googleapis.com/charts"
  chart      = "base"
  create_namespace = true
}

resource "helm_release" "istiod" {
  name       = "istiod"
  namespace  = "istio-system"
  repository = "https://istio-release.storage.googleapis.com/charts"
  chart      = "istiod"
  values     = [file("${path.module}/istio-values.yaml")]
}
________________________________________
üîπ E. Enable Sidecar Injection (Namespace YAML)
apiVersion: v1
kind: Namespace
metadata:
  name: demo
  labels:
    istio-injection: enabled
Apply:
bash
CopyEdit
kubectl apply -f namespace.yaml
Now, any pod in demo namespace will automatically have the Envoy proxy sidecar injected.
________________________________________
üîπ F. Traffic Management YAML Example (VirtualService)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: frontend-route
spec:
  hosts:
  - frontend.default.svc.cluster.local
  http:
  - route:
    - destination:
        host: frontend
        port:
          number: 80
________________________________________
üîπ G. Importance
‚Ä¢	Zero-trust security with mTLS.
‚Ä¢	Granular traffic control (routing, retries, timeouts).
‚Ä¢	Native observability with telemetry and tracing.
‚Ä¢	Enables A/B testing, canary deployments, circuit breaking.
________________________________________
üîπ H. Interviewer Summary
Istio is an advanced service mesh deployed via Helm (istiod and base charts). It enables deep control over microservice-to-microservice 
communication with features like routing, retries, security (mTLS), and observability. It‚Äôs commonly used in modern production Kubernetes 
environments for zero-trust networking and progressive delivery.
________________________________________
‚úÖ 8. Vertical Pod Autoscaler (VPA) Setup on Kubernetes
________________________________________
üöÄ Overview
Vertical Pod Autoscaler (VPA) automatically adjusts CPU and memory requests/limits for your pods based on usage.
________________________________________
üß© YAML Setup (For a Sample Deployment)
# vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: nginx
  updatePolicy:
    updateMode: "Auto"
________________________________________
üì¶ Helm Installation
There‚Äôs no official Helm chart by Kubernetes, but the VPA components can be deployed with Helm-compatible manifests:
Step 1: Clone repo and apply
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler/

Step 2: Install VPA components (recommender, updater, and admission controller)
kubectl apply -f deploy/recommender/
kubectl apply -f deploy/updater/
kubectl apply -f deploy/admission-controller/
You can modify these as Helm templates if required.
________________________________________
‚öôÔ∏è Terraform Integration
We will deploy VPA components using Terraform kubernetes_manifest or helm_release (if you convert to Helm chart).
Example: Terraform with kubernetes_manifest
resource "kubernetes_manifest" "vpa" {
  manifest = yamldecode(file("${path.module}/vpa.yaml"))
}
You can also wrap the recommender, updater, and admission-controller into Helm-compatible values if needed.
________________________________________
‚úÖ VPA + Sample Deployment YAML
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: nginx
  updatePolicy:
    updateMode: "Auto"
________________________________________
‚úÖ 9. AWS Secrets Manager CSI Driver Integration with Kubernetes
This allows Kubernetes workloads to securely fetch secrets directly from AWS Secrets Manager, using Secrets Store CSI Driver + AWS Provider.
________________________________________
üîß Step-by-Step Setup
________________________________________
üì¶ 1. Install Secrets Store CSI Driver using Helm
helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts
helm repo update

helm install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver \
  --namespace kube-system
________________________________________
üì¶ 2. Install AWS Provider for the CSI Driver
kubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml
________________________________________
üîê 3. Create IAM Policy & IAM Role (IRSA) for Your Pod
Sample IAM Policy (for AWS Secrets Manager access):
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "SecretsManagerRead",
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue"
      ],
      "Resource": "arn:aws:secretsmanager:<region>:<account-id>:secret:<secret-name>*"
    }
  ]
}
Attach this policy to an IAM role and associate it with your Kubernetes service account using IRSA.
________________________________________
üõ†Ô∏è 4. SecretProviderClass YAML (AWS Secrets Manager)
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: aws-secrets
  namespace: default
spec:
  provider: aws
  parameters:
    objects: |
      - objectName: "my-secret-name"
        objectType: "secretsmanager"
________________________________________
üß™ 5. Deployment That Uses the Secret (Volume Mount)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-secret-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-secret-app
  template:
    metadata:
      labels:
        app: nginx-secret-app
    spec:
      serviceAccountName: sa-aws-csi
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: secrets-store
          mountPath: "/mnt/secrets-store"
          readOnly: true
      volumes:
      - name: secrets-store
        csi:
          driver: secrets-store.csi.k8s.io
          readOnly: true
          volumeAttributes:
            secretProviderClass: "aws-secrets"
________________________________________
üõ°Ô∏è 6. Service Account with IRSA Annotation
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-aws-csi
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<your-irsa-role>
________________________________________
üîß Terraform Integration for AWS Secrets Manager CSI Driver
You can manage the SecretProviderClass, IAM Role, and Deployment using Terraform:
Sample Terraform (SecretProviderClass)
resource "kubernetes_manifest" "aws_secret_provider" {
  manifest = yamldecode(file("${path.module}/secretproviderclass.yaml"))
}
IAM Role + Trust Policy for IRSA (Terraform)
hcl
CopyEdit
resource "aws_iam_role" "irsa_role" {
  name = "eks-irsa-secrets-csi"
  assume_role_policy = data.aws_iam_policy_document.irsa_assume.json
}

data "aws_iam_policy_document" "irsa_assume" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"
    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.eks.arn]
    }
    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.eks.url, "https://", "")}:sub"
      values   = ["system:serviceaccount:default:sa-aws-csi"]
    }
  }
}
________________________________________
‚úÖ Summary
| Component           | Setup Tool       | Notes                    |
| ------------------- | ---------------- | ------------------------ |
| CSI Driver          | Helm             | secrets-store-csi-driver |
| AWS Provider        | kubectl manifest | Installed separately     |
| IAM Role            | Terraform        | IRSA role with policy    |
| SecretProviderClass | YAML / Terraform | Defines secret objects   |
| App Deployment      | YAML / Terraform | Mounts secrets from CSI  |
________________________________________
‚úÖ 8. Vertical Pod Autoscaler (VPA) ‚Äì Setup Guide
________________________________________
üöÄ Purpose:
The Vertical Pod Autoscaler (VPA) automatically adjusts CPU and memory requests/limits of containers based on usage to optimize resource 
utilization. It is useful for workloads with unpredictable or growing resource needs.
________________________________________
üß© 1. Helm Installation of VPA
üõ†Ô∏è Prerequisites:
‚Ä¢	Kubernetes v1.20+
‚Ä¢	Metrics server must be running.
üì¶ Install via Helm:
helm repo add fairwinds-stable https://charts.fairwinds.com/stable
helm repo update

helm install vpa fairwinds-stable/vpa \
  --namespace kube-system \
  --create-namespace
________________________________________
‚öôÔ∏è 2. Sample values.yaml for Helm (Optional Customization)
updater:
  enabled: true

recommender:
  enabled: true

admissionController:
  enabled: true

image:
  repository: k8s.gcr.io/autoscaling
  tag: latest
Save as: vpa-values.yaml
Install using:
helm install vpa fairwinds-stable/vpa -f vpa-values.yaml --namespace kube-system
________________________________________
üìÑ 3. Sample YAML to Apply VPA to Deployment
üîß vpa.yaml:
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       nginx
  updatePolicy:
    updateMode: "Auto"  # Options: "Off", "Initial", "Auto"
Apply:
kubectl apply -f vpa.yaml
________________________________________
üåê 4. Terraform Integration
You can deploy the Helm chart using Terraform's helm_release:
üîß vpa.tf
provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

resource "helm_release" "vpa" {
  name       = "vpa"
  repository = "https://charts.fairwinds.com/stable"
  chart      = "vpa"
  namespace  = "kube-system"
  create_namespace = true

  values = [file("${path.module}/vpa-values.yaml")]
}
________________________________________
| Pros                                     | Cons                                      |
| ---------------------------------------- | ----------------------------------------- |
| Auto-adjusts pod resources to real usage | Cannot be used with HPA on same resource  |
| Reduces over-provisioning                | Causes restarts (updates pod spec)        |
| Good for memory-sensitive apps           | Limited community tooling compared to HPA |
________________________________________
üß† Importance
‚Ä¢	Optimizes pod resources dynamically.
‚Ä¢	Especially useful in batch jobs or ML workloads.
‚Ä¢	Helps reduce under/over-provisioning and costs.
________________________________________
üéØ Interviewer-Style Summary
The Vertical Pod Autoscaler (VPA) helps Kubernetes pods automatically adjust their CPU and memory requests/limits based on historical and 
live usage. It uses three components: recommender, updater, and admission controller. It‚Äôs typically deployed using Helm or raw YAML, 
and is best suited for workloads that need frequent resource tuning. However, VPA is not compatible with HPA on the same container, 
so we must choose based on scaling requirements.
________________________________________
‚úÖ Component 11: Node Upgrade in Kubernetes (EKS Managed Nodes)
________________________________________
‚öôÔ∏è Objective:
Upgrade EKS nodes (managed node groups or self-managed nodes) safely using Terraform.
________________________________________
üéØ Method 1: Managed Node Group Upgrade (Terraform)
‚úÖ Step-by-Step Strategy:
1.	Use Terraform to define a managed node group.
2.	Upgrade by modifying ami_type or version.
3.	AWS automatically handles cordoning, draining, and replacing nodes.
________________________________________
üìÅ Terraform Code Example
main.tf
provider "aws" {
  region = "us-west-2"
}

module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  cluster_name    = "dev-cluster"
  cluster_version = "1.29"
  subnets         = module.vpc.private_subnets
  vpc_id          = module.vpc.vpc_id

  eks_managed_node_groups = {
    dev_nodes = {
      instance_types = ["t3.medium"]
      desired_size   = 2
      min_size       = 1
      max_size       = 3
      ami_type       = "AL2_x86_64"
      version        = "1.29"
    }
  }
}
Upgrade Steps:
‚Ä¢	To upgrade the nodes, change the version (e.g., from 1.28 to 1.29) in eks_managed_node_groups.
‚Ä¢	Run:
terraform plan
terraform apply
AWS will upgrade the nodes without downtime (cordon + drain + launch new nodes).
________________________________________
üéØ Method 2: Self-Managed Node Upgrade (Kubeadm)
If you're using kubeadm-based clusters (non-EKS), node upgrades involve OS-level operations.
‚úÖ Step-by-Step Manual Strategy:
1.	Cordon node:
kubectl cordon <node-name>

2.	Drain node:
kubectl drain <node-name> --ignore-daemonsets --delete-local-data

3.	SSH into node & upgrade kubelet/kubeadm/kubernetes components:
sudo apt-get update && \
sudo apt-get install -y kubelet=1.29.0-00 kubeadm=1.29.0-00 kubectl=1.29.0-00

sudo systemctl daemon-reexec
sudo systemctl restart kubelet

4.	Uncordon:
kubectl uncordon <node-name>
________________________________________
üõ†Ô∏è Helm / YAML:
Not applicable for node upgrades directly. This is infra-level and handled via Terraform or manual commands.
________________________________________
üìå Importance:
‚Ä¢	Regular upgrades ensure performance, feature availability, and security.
‚Ä¢	EKS managed upgrades are simpler and production-safe.
‚Ä¢	Self-managed clusters need careful handling to avoid downtime.
________________________________________
üßëüíº Interview-Style Summary:
‚ÄúTo upgrade EKS managed node groups, I use Terraform by updating the version in the eks_managed_node_groups block. AWS automatically
cordons, drains, and replaces the old nodes. For self-managed clusters using kubeadm, I cordon and drain nodes, SSH in, update Kubernetes 
binaries, restart kubelet, and then uncordon. For production, I prefer using managed node groups due to ease and safety.‚Äù
________________________________________
‚úÖ What is an Ingress Controller in Kubernetes?
Ingress Controller is a Kubernetes component that listens for Ingress resources and routes external HTTP/HTTPS traffic to the appropriate 
services inside the cluster.
‚Ä¢	Ingress Resource defines routing rules
‚Ä¢	Ingress Controller enforces those rules (e.g., NGINX, ALB, Traefik)
________________________________________
üîß 1. Installing NGINX Ingress Controller using Helm (generic)
Step 1: Add the Helm repo
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

Step 2: Install the Ingress Controller
helm install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx --create-namespace
Optional values.yaml:
controller:
  replicaCount: 2
  service:
    type: LoadBalancer
  nodeSelector:
    kubernetes.io/os: linux

Command using values.yaml:
helm install ingress-nginx ingress-nginx/ingress-nginx \
  -f values.yaml \
  --namespace ingress-nginx --create-namespace
________________________________________
üîß 2. Ingress Resource Example (for NGINX)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-service
                port:
                  number: 80
________________________________________
üîß 3. ALB Ingress Controller Setup (AWS Specific)
Prerequisites:
‚Ä¢	EKS cluster
‚Ä¢	OIDC provider configured
‚Ä¢	IAM role for service account
Step-by-step:
a) Add AWS Load Balancer Controller Helm repo:
helm repo add eks https://aws.github.io/eks-charts
helm repo update

b) Create IAM OIDC provider (only once per cluster):
eksctl utils associate-iam-oidc-provider \
  --region us-east-1 \
  --cluster my-eks-cluster \
  --approve

c) Create IAM policy:
Download and apply the IAM policy:
curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json

aws iam create-policy \
  --policy-name AWSLoadBalancerControllerIAMPolicy \
  --policy-document file://iam-policy.json

d) Create IAM role for ServiceAccount using eksctl:
eksctl create iamserviceaccount \
  --cluster=my-eks-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --attach-policy-arn=arn:aws:iam::<ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve

e) Install ALB Controller via Helm:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=my-eks-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=us-east-1 \
  --set vpcId=vpc-xxxxxxxx \
  --set ingressClass=alb
________________________________________
üîß 4. Ingress Resource for ALB:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alb-ingress
  namespace: default
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/group.name: my-group
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
spec:
  ingressClassName: alb
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-service
                port:
                  number: 80
________________________________________
üîß 5. Terraform Example for ALB Ingress Controller on EKS
You can use the helm_release block to deploy the controller:
resource "helm_release" "aws_alb_ingress" {
  name       = "aws-load-balancer-controller"
  repository = "https://aws.github.io/eks-charts"
  chart      = "aws-load-balancer-controller"
  namespace  = "kube-system"

  set {
    name  = "clusterName"
    value = var.cluster_name
  }

  set {
    name  = "serviceAccount.create"
    value = "false"
  }

  set {
    name  = "serviceAccount.name"
    value = "aws-load-balancer-controller"
  }

  set {
    name  = "region"
    value = var.region
  }

  set {
    name  = "vpcId"
    value = var.vpc_id
  }

  set {
    name  = "ingressClass"
    value = "alb"
  }
}
________________________________________
| Pros                           | Cons                                     |
| ------------------------------ | ---------------------------------------- |
| Centralized routing management | Requires setup and maintenance           |
| TLS termination support        | Different controllers behave differently |
| Path and subdomain routing     | ALB requires IAM permissions             |
| Load balancing across services | Misconfigured Ingress = traffic issues   |
________________________________________
üß† Interviewer Summary
An Ingress Controller in Kubernetes acts as an entry point for external traffic, managing routing rules defined in Ingress resources. 
We can install NGINX or ALB controllers using Helm. NGINX is suitable for basic routing, while AWS ALB Ingress integrates well with 
AWS-managed infrastructure. The controller is responsible for managing underlying LoadBalancers and routing traffic to the correct
service. It simplifies load balancing, path-based routing, and TLS termination within the cluster.
________________________________________
‚úÖ Directory Layout
alb-ingress-controller/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ provider.tf
‚îú‚îÄ‚îÄ helm/
‚îÇ   ‚îî‚îÄ‚îÄ aws-load-balancer-controller/
‚îÇ       ‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ kubernetes/
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ   ‚îú‚îÄ‚îÄ sample-app-deployment.yaml
‚îÇ   ‚îî‚îÄ‚îÄ sample-app-service.yaml
________________________________________
üìÅ 1. terraform/main.tf
Creates:
‚Ä¢	IAM OIDC provider
‚Ä¢	IAM Role + Policy for ALB Controller
‚Ä¢	EKS config (if needed)

provider "aws" {
  region = var.region
}

resource "aws_iam_policy" "alb_ingress_policy" {
  name = "AWSLoadBalancerControllerIAMPolicy"
  policy = file("${path.module}/iam_policy.json")
}

resource "aws_iam_role" "alb_controller_role" {
  name = "AmazonEKSLoadBalancerControllerRole"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRoleWithWebIdentity"
      Effect = "Allow"
      Principal = {
        Federated = var.oidc_provider_arn
      }
      Condition = {
        StringEquals = {
          "${replace(var.oidc_provider_url, "https://", "")}:sub" = "system:serviceaccount:kube-system:aws-load-balancer-controller"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "alb_policy_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_ingress_policy.arn
}
________________________________________
üìÅ 2. terraform/variables.tf
variable "region" {
  default = "us-east-1"
}

variable "oidc_provider_arn" {}
variable "oidc_provider_url" {}
________________________________________
üìÅ 3. terraform/iam_policy.json
Download this from the official AWS repo:
bash
CopyEdit
wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
________________________________________
üìÅ 4. helm/aws-load-balancer-controller/values.yaml
clusterName: your-cluster-name
region: us-east-1
vpcId: vpc-xxxxxx
serviceAccount:
  create: false
  name: aws-load-balancer-controller
________________________________________
üìÅ 5. kubernetes/sample-app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample
  template:
    metadata:
      labels:
        app: sample
    spec:
      containers:
      - name: app
        image: nginx
        ports:
        - containerPort: 80
________________________________________
üìÅ 6. kubernetes/sample-app-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sample-app
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    app: sample
________________________________________
üìÅ 7. kubernetes/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sample-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: sample-app
                port:
                  number: 80
________________________________________
‚úÖ Deployment Steps
1. Create IAM resources via Terraform
cd terraform/
terraform init
terraform apply

2. Add Helm repo and install the controller
helm repo add eks https://aws.github.io/eks-charts
helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  -f helm/aws-load-balancer-controller/values.yaml \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set clusterName=your-cluster-name

3. Deploy the app and ingress
kubectl apply -f kubernetes/
________________________________________
üìò README.md (Example)
# AWS Load Balancer Controller Setup
This repo installs the ALB Ingress Controller on EKS using Helm, Terraform, and Kubernetes manifests.

## Structure
- `terraform/`: IAM role and policy
- `helm/`: Helm values file
- `kubernetes/`: Sample app, service, and ingress

## Instructions
1. Apply Terraform to create IAM role.
2. Deploy Helm chart.
3. Apply app + ingress.
________________________________________
üîê KUBERNETES AUTHENTICATION METHODS
Authentication determines who the user or service is (e.g., human user, service account, CI/CD pipeline, etc.).
1. Static Password File (Basic Auth)
üìù Deprecated (as of Kubernetes v1.19+) ‚Äî not recommended for production.
‚Ä¢	Define usernames and passwords in a static CSV file.
‚Ä¢	Used in early Kubernetes versions for quick testing.

# Example static-password-file.csv
password123,admin,1
üî∏ Use Case: Testing environments
üî∏ Cons: Insecure, not scalable, deprecated
________________________________________
2. Static Token File
Similar to basic auth, but uses bearer tokens
‚Ä¢	CSV file with tokens provided to the API server.
# Example token.csv
abcdef1234567890,admin,1
üî∏ Use Case: CI/CD systems, temporary users
üî∏ Cons: Insecure, not rotated automatically
________________________________________
3. X.509 Client Certificates
‚Ä¢	Certificate-based authentication; user is issued a client certificate signed by a trusted CA.
‚Ä¢	The kube-apiserver validates it.
# Generate certificate
openssl genrsa -out user.key 2048
openssl req -new -key user.key -out user.csr -subj "/CN=dev-user/O=dev-team"
openssl x509 -req -in user.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out user.crt -days 365
üî∏ Use Case: Long-term secure user auth
üî∏ Pros: Strong authentication
üî∏ Cons: Manual management, no revocation
________________________________________
4. Service Account Tokens (Recommended for Pods)
‚Ä¢	Each pod automatically gets a service account token mounted at /var/run/secrets/kubernetes.io/serviceaccount/token
‚Ä¢	Used by apps inside the cluster
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app-sa
üî∏ Use Case: Pod-to-API server communication
üî∏ Pros: Auto-managed, integrated with RBAC
üî∏ Cons: Needs proper RBAC restriction
________________________________________
5. OpenID Connect (OIDC) ‚Äî ‚úÖ Production-Ready
‚Ä¢	Uses external identity providers (Google, Azure AD, Okta, Keycloak).
‚Ä¢	Best for centralized identity management.
# kube-apiserver flags (example)
--oidc-issuer-url=https://accounts.google.com \
--oidc-client-id=k8s-client \
--oidc-username-claim=email \
--oidc-groups-claim=groups
üî∏ Use Case: Enterprise SSO, GitHub login, SAML integration
üî∏ Pros: Centralized IAM, secure, scalable
üî∏ Cons: Needs OIDC provider + HTTPS setup
________________________________________
6. Webhook Token Authentication
‚Ä¢	Auth request forwarded to an external webhook.
‚Ä¢	Webhook verifies identity and returns response.
# webhook config file
apiVersion: v1
kind: Config
clusters:
  - name: webhook
    cluster:
      server: https://auth-server.local/authenticate
üî∏ Use Case: Custom logic (e.g., LDAP, 2FA)
üî∏ Pros: Fully customizable
üî∏ Cons: Adds external dependency
________________________________________
üõ°Ô∏è KUBERNETES AUTHORIZATION METHODS
Authorization determines what the authenticated user can do.
1. RBAC (Role-Based Access Control) ‚Äî ‚úÖ Default and Recommended
‚Ä¢	Grant permissions using Roles and RoleBindings
# Role (namespace scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

# RoleBinding
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
- kind: User
  name: alice
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
üî∏ Use Case: Granting fine-grained permissions
üî∏ Pros: Secure, namespace and cluster-wide
üî∏ Cons: Can become complex with many roles
________________________________________
2. ABAC (Attribute-Based Access Control)
üìù Deprecated
‚Ä¢	Uses a JSON policy file based on user/resource/verb attributes.
{"user":"dev", "namespace":"dev", "resource":"pods", "verb":"get", "allow": true}
üî∏ Use Case: Custom environments
üî∏ Cons: Not dynamic, hard to maintain
________________________________________
3. Webhook Authorization
‚Ä¢	Like authentication webhook, but for authorization.
‚Ä¢	Sends a POST request to an external server with the user and action.
# kube-apiserver
--authorization-mode=Webhook
--authorization-webhook-config-file=/etc/webhook-authz.conf
üî∏ Use Case: Integrate with external auth systems (e.g., LDAP, AD)
üî∏ Pros: Full control
üî∏ Cons: Slower, external dependency
________________________________________
4. Node Authorization
‚Ä¢	Special-purpose mode to restrict what kubelets (nodes) can access.
‚Ä¢	Automatically enabled for internal Kubernetes communication.
üî∏ Use Case: Secure kubelet access
üî∏ Pros: Built-in, secure
üî∏ Cons: Limited to nodes only
________________________________________
5. AlwaysAllow / AlwaysDeny (for testing)
Not used in production.
# kube-apiserver flag
--authorization-mode=AlwaysAllow
________________________________________
| Feature      | Authentication Methods              | Authorization Methods     |
| ------------ | ----------------------------------- | ------------------------- |
| Purpose      | Identify *who* is calling           | Define *what* they can do |
| Default      | X.509 certs, Service Account tokens | RBAC                      |
| Recommended  | OIDC, Service Accounts              | RBAC                      |
| Customizable | Webhook, OIDC                       | Webhook                   |
| Deprecated   | Basic Auth, Static Tokens           | ABAC                      |
________________________________________
‚úÖ Best Practices
‚Ä¢	Use OIDC + RBAC for production-grade setups.
‚Ä¢	Use ServiceAccounts for pod workloads.
‚Ä¢	Limit permissions by namespace and verb.
‚Ä¢	Regularly audit roles and access.
‚Ä¢	Prefer Helm/Infra as Code to define RBAC policies.

‚úÖ Kubernetes Authorization ‚Äì Explained
Kubernetes Authorization determines what actions an authenticated user or service account can perform on Kubernetes resources. 
Once a user is authenticated, Kubernetes checks authorization using one or more authorization modules.
________________________________________
üîê 1. What is Authorization in Kubernetes?
Authorization is the process of determining if an authenticated entity (user, group, or service account) has permissions to perform a 
specific verb (get, list, create, delete, etc.) on a resource (pods, deployments, etc.) within a namespace.
Authorization is enforced after authentication and before the request is processed by the Kubernetes API.
________________________________________
| Module                 | Description                                                           |
| ---------------------- | --------------------------------------------------------------------- |
| **Node Authorization** | Used internally for kubelets (nodes) to access limited API resources. |
| **RBAC**               | Role-Based Access Control, the most commonly used mechanism.          |
| **ABAC**               | Attribute-Based Access Control using policy files.                    |
| **Webhook**            | External authorization service using HTTP callbacks.                  |
| **AlwaysAllow**        | Allows all requests (not recommended for production).                 |
| **AlwaysDeny**         | Denies all requests (used for testing).                               |
________________________________________
üîé 2. Detailed Breakdown of Authorization Types
________________________________________
‚úÖ 1. Node Authorization
‚Ä¢	Purpose: Special-purpose authorizer for kubelet processes.
‚Ä¢	Use Case: Allow nodes to access secrets, configmaps, and pods assigned to them.
‚Ä¢	Automatically enabled on kube-apiserver.
--authorization-mode=Node,RBAC
‚Ä¢	Restrictive and automatically identifies nodes via certificates (system:node:<nodename> group).
‚Ä¢	You should use RBAC alongside it.
________________________________________
‚úÖ 2. RBAC (Role-Based Access Control)
‚Ä¢	Most widely used and recommended method
‚Ä¢	Grants access based on roles and bindings
üîπKey Components:
‚Ä¢	Role: Defines permissions (verbs, resources) within a namespace
‚Ä¢	ClusterRole: Same as Role, but across the whole cluster
‚Ä¢	RoleBinding: Binds a Role to users or service accounts in a namespace
‚Ä¢	ClusterRoleBinding: Binds ClusterRole across namespaces
üßæ Example:
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
________________________________________
‚úÖ 3. ABAC (Attribute-Based Access Control)
‚Ä¢	Uses JSON-formatted policy files with user, group, resource, verb attributes.
‚Ä¢	Enabled with:
--authorization-mode=ABAC --authorization-policy-file=/path/to/policy.json

üßæ Example:
{
  "apiVersion": "abac.authorization.kubernetes.io/v1beta1",
  "kind": "Policy",
  "spec": {
    "user": "dev-user",
    "namespace": "dev",
    "resource": "pods",
    "apiGroup": "",
    "verb": "get"
  }
}
‚Ä¢	Not recommended for production ‚Äî hard to manage, not dynamic like RBAC.
________________________________________
‚úÖ 4. Webhook Authorization
‚Ä¢	Sends API requests to a custom external authorization service (webhook).
‚Ä¢	The webhook server replies with "Allow" or "Deny".
‚Ä¢	Use when you have custom business logic or centralized authorization systems.
Example Config:
--authorization-mode=Webhook
--authorization-webhook-config-file=/etc/kubernetes/authz-webhook-config.yaml
Sample webhook request/response:
json
CopyEdit
// Request sent to webhook server
{
  "kind": "SubjectAccessReview",
  "spec": {
    "user": "alice",
    "resourceAttributes": {
      "namespace": "default",
      "verb": "get",
      "resource": "pods"
    }
  }
}
________________________________________
‚úÖ 5. AlwaysAllow (Not for Production)
‚Ä¢	Grants full access to everyone.
‚Ä¢	Only for testing.
--authorization-mode=AlwaysAllow
________________________________________
‚úÖ 6. AlwaysDeny (Not for Production)
‚Ä¢	Denies all requests.
‚Ä¢	Used for debugging only.
--authorization-mode=AlwaysDeny
________________________________________
üö¶ How Kubernetes Uses Authorization
1.	User is authenticated (e.g., via cert, token, etc.)
2.	Kubernetes checks each configured authorizer in order:
o	Node
o	RBAC
o	Webhook
o	etc.
3.	The first authorizer that returns Allow or Deny is used.
4.	If no authorizer allows, the request is denied.
________________________________________
| Scenario                      | Recommended Authorization |
| ----------------------------- | ------------------------- |
| Production Kubernetes cluster | **RBAC + Node**           |
| Complex enterprise policies   | **RBAC + Webhook**        |
| Kubelet/node-specific access  | **Node Authorization**    |
| Legacy/testing/dev            | ABAC / AlwaysAllow        |
________________________________________
üìå Interview Summary
‚Ä¢	Authorization in Kubernetes ensures that users and service accounts have the right level of access to resources.
‚Ä¢	The RBAC system is the most used and recommended authorization mode.
‚Ä¢	Multiple authorizers can be chained and evaluated in order.
‚Ä¢	Kubernetes supports custom logic via Webhook authorization.
‚Ä¢	Misconfigured or overly permissive authorization can lead to security vulnerabilities.
________________________________________
| Authorization Mode     | Description                                                            |
| ---------------------- | ---------------------------------------------------------------------- |
| `Node`                 | Grants permissions to kubelets.                                        |
| `RBAC`                 | Role-Based Access Control using `Role`, `ClusterRole`, and `Bindings`. |
| `ABAC`                 | Attribute-Based Access Control (requires a JSON policy file).          |
| `Webhook`              | External authorization via HTTP webhook.                               |
| `AlwaysAllow` / `Deny` | For testing only, allows or denies all requests.                       |
________________________________________
‚úÖ Step-by-Step with YAMLs: Focus on RBAC (the most widely used)
________________________________________
‚úÖ 1. RBAC - Role Based Access Control
RBAC uses:
‚Ä¢	Role / ClusterRole
‚Ä¢	RoleBinding / ClusterRoleBinding
‚Ä¢	ServiceAccount
________________________________________
üîπ Example 1: Role and RoleBinding (Namespace-specific)
üßæ role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

üßæ rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
________________________________________
üîπ Example 2: ClusterRole and ClusterRoleBinding (Cluster-wide access)
üßæ clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin-readonly
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

üßæ clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-readonly-binding
subjects:
- kind: User
  name: readonly-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin-readonly
  apiGroup: rbac.authorization.k8s.io
________________________________________
üîπ Example 3: Bind ServiceAccount to Role (for Pods)
üßæ serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-pod-reader
  namespace: dev

üßæ rolebinding-sa.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sa-pod-reader-binding
  namespace: dev
subjects:
- kind: ServiceAccount
  name: sa-pod-reader
  namespace: dev
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
________________________________________
üßæ Optional: Create User Certificates (for RoleBinding)
To bind users like dev-user or readonly-user, you typically create TLS certificates and pass them via kubectl config:
kubectl config set-credentials dev-user --client-certificate=dev-user.crt --client-key=dev-user.key
kubectl config set-context dev-context --cluster=my-cluster --user=dev-user
kubectl config use-context dev-context
________________________________________
| Mode               | YAML Support | Notes                                                                 |
| ------------------ | ------------ | --------------------------------------------------------------------- |
| ABAC               | ‚ùå            | Uses static JSON policy file passed via `--authorization-policy-file` |
| Node               | ‚ùå            | Automatically assigned to kubelets                                    |
| Webhook            | ‚ùå            | External, configured via flags                                        |
| AlwaysAllow / Deny | ‚ùå            | Not used in production                                                |
________________________________________
‚úÖ Kubernetes Authorization Types ‚Äì Overview
Kubernetes supports three main types of authorization modules to control access to the Kubernetes API:
________________________________________
1. Node Authorization
‚Ä¢	Purpose: Grants permissions to kubelets (nodes) to interact with resources they need (like pods on that node).
‚Ä¢	Use case: Automatically used when kubelets access the API server.
‚Ä¢	How it works: Evaluates if a request is from a node and allows access only to resources associated with that node.
________________________________________
2. Attribute-Based Access Control (ABAC)
‚Ä¢	Purpose: Legacy support system that uses static JSON policy files.
‚Ä¢	Use case: Not recommended for production; lacks flexibility.
‚Ä¢	How it works: Checks incoming request attributes (user, verb, resource, etc.) against policies in a static file.
Example JSON Policy:
{
  "apiVersion": "abac.authorization.kubernetes.io/v1beta1",
  "kind": "Policy",
  "spec": {
    "user": "dev-user",
    "namespace": "default",
    "resource": "pods",
    "apiGroup": "*",
    "verb": "get"
  }
}
________________________________________
3. Role-Based Access Control (RBAC) ‚úÖ Recommended
‚Ä¢	Purpose: Fine-grained, dynamic authorization system.
‚Ä¢	Use case: Most widely used in production clusters.
‚Ä¢	How it works: Grants users or groups access based on Roles and RoleBindings (or ClusterRole + ClusterRoleBinding).
________________________________________
üßæ RBAC YAML Examples (Step-by-Step)
________________________________________
a. Create Role (namespace-scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
________________________________________
b. Bind Role to User
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
- kind: User
  name: dev-user  # the Kubernetes user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
________________________________________
c. ClusterRole (cluster-wide permissions)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: admin-cluster-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
________________________________________
d. Bind ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-binding
subjects:
- kind: User
  name: cluster-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin-cluster-role
  apiGroup: rbac.authorization.k8s.io
________________________________________
4. Webhook Authorization
‚Ä¢	Purpose: Delegates authorization decisions to an external service.
‚Ä¢	Use case: Custom policies based on your business logic.
‚Ä¢	How it works: API server sends a webhook request to an external authorizer API.
Example webhook request:
{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "spec": {
    "user": "john",
    "resourceAttributes": {
      "namespace": "dev",
      "verb": "get",
      "resource": "pods"
    }
  }
}
________________________________________
| Authorization Type | Scope             | Dynamic? | Recommended? | Notes                   |
| ------------------ | ----------------- | -------- | ------------ | ----------------------- |
| Node               | Node-specific     | Yes      | ‚úÖ            | Automatic for kubelets  |
| ABAC               | Cluster-wide      | No       | ‚ùå            | Deprecated in most use  |
| RBAC               | Namespace/Cluster | Yes      | ‚úÖ            | Most common & secure    |
| Webhook            | External System   | Yes      | ‚ö†Ô∏è           | Advanced custom control |
________________________________________
‚úÖ What is kubeconfig?
The kubeconfig file is used by kubectl and other Kubernetes clients to:
‚Ä¢	Authenticate (who you are),
‚Ä¢	Authorize (what you can do),
‚Ä¢	Target the correct cluster and context.
________________________________________
üîê kubeconfig - Key Components
apiVersion: v1
kind: Config
clusters:
- name: my-cluster
  cluster:
    server: https://<API_SERVER_ENDPOINT>
    certificate-authority-data: <BASE64_CA_CERT>

users:
- name: my-user
  user:
    token: <BEARER_TOKEN>
    # or
    username: admin
    password: secret
    # or
    client-certificate-data: <BASE64_CERT>
    client-key-data: <BASE64_KEY>

contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: default

current-context: my-context
________________________________________
| Method               | Description                                                       |
| -------------------- | ----------------------------------------------------------------- |
| `token`              | Used for service accounts or OIDC-based IAM tokens (EKS)          |
| `username/password`  | Basic auth (not recommended)                                      |
| `client-certificate` | For mTLS-based auth                                               |
| `exec`               | For dynamically retrieving credentials (e.g., AWS CLI or GCP SDK) |
| `auth-provider`      | Deprecated; replaced by `exec`                                    |
| `aws eks get-token`  | Used by AWS EKS to get short-lived tokens via IAM                 |

üìÅ GitHub-Ready Directory Structure for Auth Setup
k8s-auth/
‚îú‚îÄ‚îÄ kubeconfig/
‚îÇ   ‚îú‚îÄ‚îÄ basic-auth-config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ cert-auth-config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ eks-iam-auth-config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ sa-token-config.yaml
‚îú‚îÄ‚îÄ authn-methods/
‚îÇ   ‚îú‚îÄ‚îÄ serviceaccount-token.yaml
‚îÇ   ‚îú‚îÄ‚îÄ client-cert-generation.sh
‚îú‚îÄ‚îÄ authz-methods/
‚îÇ   ‚îú‚îÄ‚îÄ rbac-role.yaml
‚îÇ   ‚îú‚îÄ‚îÄ rbac-rolebinding.yaml
‚îÇ   ‚îú‚îÄ‚îÄ abac-policy.json
‚îÇ   ‚îú‚îÄ‚îÄ webhook-auth-config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ node-auth.yaml
________________________________________
‚úÖ kubeconfig for AWS EKS (example)
aws eks update-kubeconfig --region us-east-1 --name my-eks-cluster
Behind the scenes, this populates ~/.kube/config with:
users:
- name: arn:aws:iam::111122223333:role/EKSAdminRole
  user:
    exec:
      apiVersion: "client.authentication.k8s.io/v1beta1"
      command: "aws"
      args:
        - "eks"
        - "get-token"
        - "--cluster-name"
        - "my-eks-cluster"
________________________________________
üß† Summary
‚Ä¢	The kubeconfig file is where all auth/authz for kubectl is defined.
‚Ä¢	It supports multiple users, clusters, and contexts.
‚Ä¢	Works with all types of authentication: certs, tokens, IAM, SA, exec plugins, etc.
‚Ä¢	Required for interacting securely with any Kubernetes cluster.

What is that default kubeconfig file
ChatGPT said:
‚úÖ What Is the Default kubeconfig File in Kubernetes?
The default kubeconfig file is the configuration file used by the kubectl CLI to interact with a Kubernetes cluster. It stores cluster 
connection info, user credentials, and context (i.e., which cluster and namespace to use).
________________________________________
üìå Default Location:
~/.kube/config
‚Ä¢	~ refers to the current user's home directory.
‚Ä¢	On Linux/Mac: /home/username/.kube/config
‚Ä¢	On Windows: C:\Users\username\.kube\config
________________________________________
üß† Purpose of the kubeconfig File
The kubeconfig file allows kubectl to:
‚Ä¢	Connect to a Kubernetes API server
‚Ä¢	Use TLS certificates or tokens for authentication
‚Ä¢	Identify the correct namespace
‚Ä¢	Select the correct user/cluster/context
________________________________________
üß© Example kubeconfig File
apiVersion: v1
kind: Config

clusters:
- name: my-cluster
  cluster:
    server: https://<api-server-endpoint>
    certificate-authority-data: <base64-encoded-CA-cert>

users:
- name: my-user
  user:
    client-certificate-data: <base64-cert>
    client-key-data: <base64-key>

contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: default

current-context: my-context
________________________________________
üîÅ How kubectl Uses It
When you run:
kubectl get pods
kubectl reads the current context from ~/.kube/config, looks up:
‚Ä¢	The cluster's API server endpoint
‚Ä¢	The user's credentials
‚Ä¢	The namespace (default if not specified)
Then it makes the request to the API server securely.
________________________________________
üì• Auto-Generated by Cloud Providers
‚Ä¢	EKS: After running aws eks update-kubeconfig --name <cluster-name>, AWS updates this file.
‚Ä¢	GKE: Google Cloud does the same via gcloud container clusters get-credentials.
‚Ä¢	AKS: Azure via az aks get-credentials.
________________________________________
üõ†Ô∏è You Can Also Manually Create or Merge Configs
You can use the KUBECONFIG environment variable to merge multiple kubeconfigs:

export KUBECONFIG=~/.kube/config:/path/to/another/config
kubectl config view --merge
________________________________________
| Property     | Description                                        |
| ------------ | -------------------------------------------------- |
| Default path | `~/.kube/config`                                   |
| Used by      | `kubectl` and client tools                         |
| Contains     | Clusters, users, contexts, and authentication info |
| Managed by   | You, or tools like `aws eks update-kubeconfig`     |
________________________________________
‚úÖ Does It Work with an AWS IAM User?
Yes, it works ‚Äî but only if the IAM user is explicitly authorized in the EKS cluster via the aws-auth ConfigMap.
________________________________________
üîê Background: How AWS Auth Works in EKS
EKS does not natively use Kubernetes RBAC for identity management. Instead, it integrates with AWS IAM through a special ConfigMap:
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapUsers: |
    - userarn: arn:aws:iam::111122223333:user/my-user
      username: my-user
      groups:
        - system:masters
To allow an IAM user (e.g., arn:aws:iam::123456789012:user/pooja) to access the cluster:

üõ†Ô∏è Step-by-Step: Enable Access for IAM User
1. Update aws-auth ConfigMap
You must edit the aws-auth ConfigMap in the cluster and add your IAM user:
kubectl edit configmap aws-auth -n kube-system

Add under mapUsers:
  mapUsers: |
    - userarn: arn:aws:iam::123456789012:user/pooja
      username: pooja
      groups:
        - system:masters
‚úÖ This gives full admin access. You can restrict to limited groups like system:read-only if needed.

2. Update kubeconfig with the IAM User Profile
Use the AWS CLI profile associated with the IAM user:
aws eks update-kubeconfig \
  --name my-cluster \
  --region us-west-2 \
  --profile my-iam-user
This updates ~/.kube/config to include the right cluster and authentication info.

3. Test Access
kubectl get nodes
If aws-auth was updated correctly, and your IAM user has sufficient permissions (e.g., eks:DescribeCluster), it will work.
________________________________________
üîÅ Behind the Scenes (Authentication Flow)
1.	kubectl calls aws eks get-token using your AWS profile.
2.	The IAM user gets a signed token (valid for 15 minutes).
3.	That token is used to authenticate to the EKS API server.
4.	The API server consults the aws-auth ConfigMap to authorize the IAM identity.
________________________________________
| Area                    | Notes                                                       |
| ----------------------- | ----------------------------------------------------------- |
| Authentication          | Must use `aws eks get-token` (IAM auth)                     |
| Authorization           | Controlled via `aws-auth` ConfigMap                         |
| Token Expiry            | Token is short-lived (15 mins); auto-refreshed by `kubectl` |
| Not Native K8s Identity | EKS uses IAM as an auth source, not K8s native users        |
________________________________________
‚úÖ Summary
‚Ä¢	Yes, you can update the kubeconfig to use an AWS IAM user.
‚Ä¢	But that IAM user must be added to the aws-auth ConfigMap.
‚Ä¢	Then you can use the aws eks update-kubeconfig --profile <iam-user-profile> to configure access.
‚Ä¢	Kubernetes authentication happens via a signed token generated by AWS.
________________________________________
‚úÖ 1. What is the default kubeconfig file?
‚Ä¢	It is typically located at:
~/.kube/config
‚Ä¢	This file contains cluster details, user credentials, and context, allowing the kubectl CLI to interact with the cluster.
________________________________________
‚úÖ 2. Authentication via AWS IAM (aws eks update-kubeconfig)
When using:
aws eks update-kubeconfig --region <region> --name <cluster-name>

It updates the kubeconfig to use AWS IAM credentials. It adds an entry like:
users:
- name: arn:aws:iam::<ACCOUNT_ID>:user/<USERNAME>
  user:
    exec:
      apiVersion: "client.authentication.k8s.io/v1beta1"
      command: "aws"
      args:
        - "eks"
        - "get-token"
        - "--region"
        - "<region>"
        - "--cluster-name"
        - "<cluster-name>"
‚úÖ This enables IAM-based authentication using aws eks get-token.
Authentication ‚úÖ ‚Äî IAM user is verified via AWS STS.
________________________________________
‚úÖ 3. Authorization in EKS via aws-auth ConfigMap
Even if IAM authentication works, authorization is determined by whether the IAM identity is mapped to a Kubernetes RBAC user/group in 
the aws-auth ConfigMap:

apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapUsers: |
    - userarn: arn:aws:iam::<ACCOUNT_ID>:user/<USERNAME>
      username: <USERNAME>
      groups:
        - system:masters
Authorization ‚úÖ ‚Äî Kubernetes allows this user if mapped to a group like system:masters.
________________________________________
‚úÖ Summary: How kubeconfig enables both authentication and authorization
| Step                   | Role                                   |
| ---------------------- | -------------------------------------- |
| `kubeconfig` entry     | Enables `aws eks get-token` auth       |
| AWS IAM (STS token)    | Handles **authentication**             |
| `aws-auth` ConfigMap   | Maps IAM to RBAC roles (authorization) |
| `kubectl` uses context | Ties user + cluster + namespace        |
________________________________________
‚úÖ Real Use Case
‚Ä¢	You create an EKS cluster.
‚Ä¢	You add an IAM user to aws-auth ConfigMap.
‚Ä¢	You update the kubeconfig using aws eks update-kubeconfig.
‚Ä¢	Now you can kubectl get pods, etc., because you're authenticated via AWS IAM and authorized via aws-auth.
________________________________________
üîê BONUS: RBAC Roles
Even after aws-auth, fine-grained access (e.g., read-only to namespace) is done via:
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-read-only
  namespace: dev
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bind-dev-user
  namespace: dev
subjects:
- kind: User
  name: <USERNAME>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-read-only
  apiGroup: rbac.authorization.k8s.io
________________________________________
| Method                    | Used For            | Suitable For          | Description                                                    |
| ------------------------- | ------------------- | --------------------- | -------------------------------------------------------------- |
| **Static Password/File**  | Human users         | Dev only              | Basic auth using static files (not recommended for production) |
| **Client Certificates**   | Human/Service users | Dev and Prod          | X.509 certs signed by cluster CA                               |
| **Bearer Tokens**         | Service accounts    | Dev and Prod          | JWT token used by service accounts and OIDC                    |
| **OpenID Connect (OIDC)** | Human users         | Production            | Federated identity (e.g., AWS Cognito, Google, Azure AD)       |
| **Webhook Token Auth**    | External auth       | Production            | Delegates auth to custom service                               |
| **AWS IAM Authenticator** | Human users         | Production (EKS)      | Maps IAM users/roles to Kubernetes users                       |
| **Cloud IAM Integration** | Human users         | Production (GKE, AKS) | Cloud-native IAM integration                                   |
________________________________________
üëÆ Authorization in Kubernetes
Authorization defines what an authenticated user/service is allowed to do.
| Authorization Mode                   | Description                                                    |
| ------------------------------------ | -------------------------------------------------------------- |
| **RBAC** (Role-Based Access Control) | Most widely used; defines roles and binds them to users/groups |
| **ABAC** (Attribute-Based)           | Deprecated in favor of RBAC                                    |
| **Webhook Authorization**            | Delegates decisions to external service                        |
| **Node Authorization**               | Used by kubelets to access limited resources                   |
| **AlwaysAllow / AlwaysDeny**         | Dev/test only; insecure                                        |
________________________________________
üîÑ Combined Auth Flow Example
1.	üîê Authentication:
o	A developer logs in using OIDC via kubectl and gets a bearer token.
o	A service account inside a pod uses a JWT token (automatically mounted by Kubernetes).
2.	üëÆ Authorization:
o	Kubernetes checks if the user is allowed (via RBAC RoleBinding) to access a resource.
o	For service accounts, a RoleBinding or ClusterRoleBinding grants specific access.
________________________________________
üèóÔ∏è Typical Authentication & Authorization in Dev vs Prod
| Environment | Authentication Method       | Authorization Method | Notes                             |
| ----------- | --------------------------- | -------------------- | --------------------------------- |
| **Dev**     | Static token / client certs | RBAC / AlwaysAllow   | Simplified, often permissive      |
| **Prod**    | OIDC / IAM Auth / Certs     | RBAC / Webhook       | Strict and audited access control |
________________________________________
üéØ Use Cases
1. Dev Environment - Static User
# Create basic static user via kubeconfig
apiVersion: v1
kind: Config
users:
- name: dev-user
  user:
    client-certificate: /path/dev.crt
    client-key: /path/dev.key

2. RBAC for a Developer
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: developer-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
  namespace: dev
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer-role
  apiGroup: rbac.authorization.k8s.io
3. OIDC Setup Example (Production)
You configure OIDC in API server flags:
bash
CopyEdit
--oidc-issuer-url=https://accounts.google.com \
--oidc-client-id=YOUR_CLIENT_ID \
--oidc-username-claim=email
Then use RBAC to authorize OIDC users.
________________________________________
‚öôÔ∏è In EKS (Production)
‚Ä¢	Authentication: Handled via aws-iam-authenticator
‚Ä¢	IAM roles mapped to Kubernetes users in aws-auth ConfigMap.
‚Ä¢	Example:
mapRoles: |
  - rolearn: arn:aws:iam::111122223333:role/EKSAdmin
    username: eks-admin
    groups:
      - system:masters
‚Ä¢	Authorization: Controlled using Kubernetes RBAC.
________________________________________
üîê Pod/Service Account Authentication
Every pod gets a service account token mounted by default:
‚Ä¢	Token is a JWT, used to access API server.
‚Ä¢	Example RBAC for a service account:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: app
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-role
  namespace: app
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-sa-binding
  namespace: app
subjects:
- kind: ServiceAccount
  name: app-sa
roleRef:
  kind: Role
  name: app-role
  apiGroup: rbac.authorization.k8s.io
________________________________________
| Layer  | Authentication                   | Authorization       |
| ------ | -------------------------------- | ------------------- |
| Human  | OIDC, IAM, Certs                 | RBAC                |
| Pod    | Service Account JWT              | RBAC                |
| System | Webhook, IAM, Cloud Integrations | RBAC, Node, Webhook |
________________________________________
üîê Kubernetes Authentication
üìå What is Authentication?
Authentication is the process of verifying the identity of a user, service, or machine trying to interact with the Kubernetes API.
________________________________________
| Method                          | Description                                                | Suitable For         | Dev | Prod |
| ------------------------------- | ---------------------------------------------------------- | -------------------- | --- | ---- |
| **Static Token File**           | Simple token-based file used for quick auth                | Demos, tests         | ‚úÖ   | ‚ùå    |
| **Static Basic Auth File**      | Username/password stored in a flat file                    | Rarely used          | ‚úÖ   | ‚ùå    |
| **X.509 Client Certificates**   | Client presents TLS certificate                            | Secure & strong auth | ‚úÖ   | ‚úÖ    |
| **Service Accounts**            | Automatically assigned tokens for Pods                     | Internal Pod auth    | ‚úÖ   | ‚úÖ    |
| **OIDC (OpenID Connect)**       | Integrates with identity providers (Google, Okta, AWS IAM) | Enterprise SSO       | ‚ùå   | ‚úÖ    |
| **Webhook Token Authenticator** | Delegates auth to external service via webhook             | Custom integrations  | ‚ùå   | ‚úÖ    |
| **AWS IAM Authenticator**       | Auth using AWS IAM roles/users                             | EKS Production auth  | ‚úÖ   | ‚úÖ    |
________________________________________
‚úÖ Development Environment ‚Äì Authentication Methods
‚Ä¢	Use kubeconfig with:
o	X.509 Client Certs (generated using kubeadm or kubectl)
o	Basic auth or static token (quick test only)
‚Ä¢	kubectl proxy for local access
‚Ä¢	ServiceAccounts in dev namespace for in-cluster workloads
________________________________________
‚úÖ Production Environment ‚Äì Authentication Methods
‚Ä¢	OIDC (with AWS IAM, Okta, Auth0, Google Workspace)
‚Ä¢	ServiceAccounts with IAM Roles for Service Account (IRSA)
‚Ä¢	X.509 certificates signed by internal PKI
‚Ä¢	Enable WebhookTokenAuthenticator for central auth services
________________________________________
üîí Kubernetes Authorization
üìå What is Authorization?
Authorization is the process of deciding whether an authenticated identity can perform a given action (like get, create, or delete) 
on a resource.
________________________________________
‚úÖ Authorization Types in Kubernetes
| Type                       | Description                                        | Use Case                  | Dev | Prod |
| -------------------------- | -------------------------------------------------- | ------------------------- | --- | ---- |
| **RBAC**                   | Role-Based Access Control using roles and bindings | Fine-grained access       | ‚úÖ   | ‚úÖ    |
| **ABAC**                   | Attribute-Based Access Control via policies        | Legacy, deprecated        | ‚úÖ   | ‚ùå    |
| **Webhook Authorization**  | External service decides access                    | Enterprise policy engines | ‚ùå   | ‚úÖ    |
| **Node Authorization**     | Special node identity auth                         | Used by kubelets          | ‚úÖ   | ‚úÖ    |
| **AlwaysAllow/AlwaysDeny** | Static allow/deny (testing only)                   | Never for prod            | ‚úÖ   | ‚ùå    |
________________________________________
‚úÖ Dev Environment ‚Äì Authorization
‚Ä¢	Use RBAC
‚Ä¢	Create minimal roles for devs using Role and RoleBinding
‚Ä¢	Default admin, edit, and view cluster roles
‚Ä¢	Namespace isolation via separate roles per namespace
________________________________________
‚úÖ Prod Environment ‚Äì Authorization
‚Ä¢	Strict RBAC with:
o	Namespaced roles for apps
o	ClusterRoles for infra
‚Ä¢	GitOps or Terraform-managed RoleBindings
‚Ä¢	Use OPA/Gatekeeper or Kyverno for policy-as-code (Webhook auth)
‚Ä¢	Least-privilege principle applied
________________________________________
üìÅ YAML Examples for Production RBAC
1. Role (Namespace Scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

2. RoleBinding (Binding Role to User)
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
- kind: User
  name: dev-user@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

3. ClusterRole (Across All Namespaces)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list"]

4. ClusterRoleBinding (for AWS IAM/OIDC User)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-oidc-binding
subjects:
- kind: User
  name: "arn:aws:iam::123456789012:user/k8s-admin"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
________________________________________
üìÅ kubeconfig and AWS IAM Integration
‚Ä¢	~/.kube/config is the default config file for kubectl

‚Ä¢	For AWS EKS:
aws eks --region us-east-1 update-kubeconfig --name prod-cluster
This will configure your kubeconfig with:
o	Cluster endpoint
o	IAM-auth token fetcher (aws-iam-authenticator)
o	Current user identity
________________________________________
üîê IRSA (IAM Roles for Service Accounts)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-reader
  namespace: default
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/S3ReadOnlyRole
This authorizes pods to assume the IAM role securely using EKS‚Äôs OIDC provider.
________________________________________
| Category   | Dev                                     | Prod                                          |
| ---------- | --------------------------------------- | --------------------------------------------- |
| Auth       | kubeconfig, static tokens, client certs | IAM, OIDC, IRSA, Webhook                      |
| AuthZ      | Simple RBAC, default roles              | Strict RBAC, ClusterRoles, Gatekeeper, Policy |
| kubeconfig | local-only                              | centralized or role-assumed                   |
| Identity   | local users/devs                        | IAM/OIDC/SSO                                  |

