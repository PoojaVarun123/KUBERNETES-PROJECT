=============================================================================================
                                        TAINTS
=============================================================================================
1. Reserve Nodes for Specific Workloads
Scenario: You want only database pods to run on certain nodes.
Taint Command: kubectl taint node db-node dedicated=database:NoSchedule
Why: This prevents non-database pods from being scheduled on db-node.
Matching Pod YAML:
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"
-------------------------------------------------------------------------------------------------------------
2. Prevent Pods from Running on Control Plane Node
Scenario: Avoid scheduling application pods on the control plane/master.
Taint (usually auto-applied): kubectl taint node master-node node-role.kubernetes.io/control-plane=:NoSchedule
Why: Protects critical Kubernetes components from being disturbed by user workloads.
-------------------------------------------------------------------------------------------------------------
3. Run Pods Only on GPU Nodes
Scenario: Schedule only ML or AI pods on GPU-enabled nodes.
Taint: kubectl taint node gpu-node hardware=gpu: NoSchedule
Why: Avoids wasting expensive GPU nodes on general workloads.
Matching Pod YAML:
tolerations:
  - key: "hardware"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
-------------------------------------------------------------------------------------------------------------
4. Taint Spot Instances to Allow Only Tolerant Pods
Scenario: Only stateless/fault-tolerant pods should run on spot instances.
Taint: kubectl taint node spot-node lifecycle=spot: NoSchedule
Why: Critical/stateful apps should not run on unpredictable nodes.
-------------------------------------------------------------------------------------------------------------
5. Dedicated Nodes for Monitoring/Logging Agents
Scenario: Fluent Bit, Prometheus, or Loki agents should run only on logging nodes.
Taint:kubectl taint node log-node monitoring=true: NoSchedule
Why: Avoids interfering with other workloads and ensures resource availability for monitoring.
-------------------------------------------------------------------------------------------------------------
6. Maintenance or Degraded Node
Scenario: Temporarily evict all pods from a node for maintenance.
Taint:kubectl taint node node-5 maintenance=true: NoExecute
Why:Evicts pods immediately unless they tolerate it. Useful for rebooting/updating a node.
Matching Pod YAML (optional delay before eviction):
tolerations:
  - key: "maintenance"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
    tolerationSeconds: 60
-------------------------------------------------------------------------------------------------------------
7. Multi-Tenant Isolation
Scenario: Team A’s workloads should not mix with Team B’s on the same node.
Taint for Team A nodes:kubectl taint node team-a-node team=team-a: NoSchedule
Matching Toleration in Team A’s Pods:
tolerations:
  - key: "team"
    operator: "Equal"
    value: "team-a"
    effect: "NoSchedule"
-------------------------------------------------------------------------------------------------------------
8. Use PreferNoSchedule for Soft Blocking
Scenario: You want to prefer not to schedule on a node but allow it if no other choice.
Taint: kubectl taint node node7 prefer=special: PreferNoSchedule
Why: Soft control — node is used only when needed.
-------------------------------------------------------------------------------------------------------------
SUMMARY:
          -Dedicated DB nodes > dedicated=database:NoSchedule Block non-DB pods
         -Control plane node > node-role.kubernetes.io/control-plane=:NoSchedule Protect master
         -GPU workloads > only hardware=gpu:NoSchedule Restrict general workloads
         -Spot nodes > lifecycle=spot:NoSchedule Prevent critical pods
         -Logging nodes >  monitoring=true:NoSchedule Dedicated for observability
         -Maintenance > maintenance=true:NoExecute Evict pods during upgrade
         -Team A nodes  > team=team-a:NoSchedule Tenant/workload isolation
         -Soft avoidance  > prefer=special:PreferNoSchedule Prefer but allow pods

===============================================================================================
1. NoSchedule – Strictly Prevent New Pods
What it does: Prevents new pods from being scheduled on the node unless they have a matching toleration.
Existing pods will keep running.
Use Cases for NoSchedule:
  -Reserve node for DB only, > Taint as dedicated=database: NoSchedule to block all other workloads
⚡️ -GPU or high-cost node > Prevent normal apps from using expensive nodes
  -Sensitive zone  > Protect compliance nodes — only specific trusted pods can run
  -Dev/prod separation > Stop prod pods from running on dev-labeled nodes
  -Node draining or testing > Temporarily block new pods during node maintenance without affecting running ones

🔄 Example:
kubectl taint node node1 dedicated=database:NoSchedule
Pod must have this toleration to be scheduled:
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"
-------------------------------------------------------------------------------------------------------------
2. NoExecute – Evict Existing Pods + Block New Pods
What it does: 
  -Immediately evicts running pods that don’t tolerate the taint
  -Prevents new pods unless they tolerate it

Use Cases for NoExecute:
  -Node failure or unresponsive > Evict all pods to reschedule elsewhere
  -Draining a node safely > Force pods to move away without kubectl drain
  -Node maintenance or upgrades > Cleanly evict pods during system patching
  -Temporary quarantine > Block and evict pods from "dirty" nodes until fixed

🕐 Control eviction delay:
tolerations:
  - key: "maintenance"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
    tolerationSeconds: 60

This lets the pod stay for 60 seconds before eviction.
-------------------------------------------------------------------------------------------------------------
3. PreferNoSchedule – Soft Block
What it does: 
  -Avoids scheduling pods on tainted nodes if possible, but not strictly.
  -If no other choice, the pod can still be scheduled.

Use Cases for PreferNoSchedule:
  -Load balancing hint: > Suggest K8s avoid overloading certain nodes
  -Use expensive nodes last > Prefer not to use GPU or high-cost nodes unless needed
  -Under-maintenance node (but not critical) > Hint scheduler to deprioritize node
  -Node with flaky hardware > Use as a last resort only

Example:
kubectl taint node node1 maintenance=soft:PreferNoSchedule
Scheduler will try not to use that node, but will if nothing else is available.

